{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy model as a webservice on Azure Container Instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "1. [Prerequisites](#prerequisites)\n",
    "\n",
    "2. [Initialize workspace](#workspace)\n",
    "\n",
    "3. [Deploy Model to ACI](#deploymodel)\n",
    "\n",
    "- a) [Create scoring file](#scoringfile)\n",
    "- b) [Define Enviroment](#env)\n",
    "- c) [Deployment configuration](#configfile)\n",
    "- d  [Deploy Webservice](#webservice)\n",
    "- e) [Test Webservice](#testservice)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Prerequisites <a id='prerequisites'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import azureml.core\n",
    "\n",
    "# display the core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize workspace <a id='workspace'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subscription_id = \"4a571c1c-a483-4a43-9930-490479d70db0\"\n",
    "resource_group = \"Learn_MLOps\"\n",
    "workspace = \"MLOs_WS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a handle to the workspace\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), subscription_id, resource_group, workspace\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deploy model <a id='deploymodel'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) Create a scoring script <a id='scoringfile'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = ml_client.models.download(name='scaler',version='1',download_path='./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc_model = ml_client.models.download(name='support-vector-classifier',version='1', download_path='./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile score.py\n",
    "import json\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import joblib\n",
    "import onnxruntime\n",
    "import time\n",
    "\n",
    "def init():\n",
    "    global model, scaler, input_name, label_name\n",
    "    # , inputs_dc, prediction_dc\n",
    "    \n",
    "\n",
    "    scaler_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model/scaler/scaler.pkl')\n",
    "    # deserialize the model file back into a sklearn model\n",
    "    scaler = joblib.load(scaler_path)\n",
    "    \n",
    "    model_onnx = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'model/support-vector-classifier/svc.onnx')\n",
    "    # print(os.listdir(model_onnx))\n",
    "    model = onnxruntime.InferenceSession(model_onnx, None)\n",
    "    input_name = model.get_inputs()[0].name\n",
    "    label_name = model.get_outputs()[0].name\n",
    "    \n",
    " \n",
    "def run(data):\n",
    "    try: \n",
    "        data = json.loads(data)[\"data\"]\n",
    "        data = np.array(data)\n",
    "        data = scaler.fit_transform(data.reshape(1, 6))\n",
    "\n",
    "        # model inference\n",
    "        result = model.run([label_name], {input_name: data.astype(np.float32)})[0]\n",
    "        # this call is saving model output data into Azure Blob\n",
    "\n",
    "    except Exception as e:   \n",
    "        result = 'error'\n",
    "        print(e)\n",
    "\n",
    "    return result.tolist()       \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the endpoint and deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
    "import datetime\n",
    "\n",
    "# online_endpoint_name = \"endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "online_endpoint_name = 'nerdward-endpoint-1'\n",
    "\n",
    "# create an online endpoint\n",
    "endpoint = ManagedOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    description=\"this is a sample online endpoint\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\"foo\": \"bar\"},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7f5b1f428820>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_client.online_endpoints.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(name='weather-aci-prediction',path='./model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2 = Model(name='support-vector-classifier')\n",
    "env = Environment(\n",
    "    conda_file=\"./model/conda.yml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi3.1.2-ubuntu18.04:latest\",\n",
    ")\n",
    "\n",
    "blue_deployment = ManagedOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model1,\n",
    "    environment=env,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"./\", scoring_script=\"score.py\"\n",
    "    ),\n",
    "    instance_type=\"Standard_F2s_v2\",\n",
    "    instance_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Check: endpoint nerdward-endpoint-1 exists\n",
      "\u001b[32mUploading 06_Model_Deployment (0.43 MBs): 100%|██████████| 427361/427361 [00:00<00:00, 6978874.15it/s]\n",
      "\u001b[39m\n",
      "\n",
      "data_collector is not a known attribute of class <class 'azure.ai.ml._restclient.v2022_02_01_preview.models._models_py3.ManagedOnlineDeployment'> and will be ignored\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7f5b1f13de80>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..........................................."
     ]
    }
   ],
   "source": [
    "ml_client.online_deployments.begin_create_or_update(blue_deployment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7f5b1f1c8250>"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# blue deployment takes 100 traffic\n",
    "endpoint.traffic = {\"blue\": 100}\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_endpoints.get(name=online_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instance status:\n",
      "SystemSetup: Succeeded\n",
      "UserContainerImagePull: Succeeded\n",
      "ModelDownload: Succeeded\n",
      "UserContainerStart: Succeeded\n",
      "\n",
      "Container events:\n",
      "Kind: Pod, Name: Pulling, Type: Normal, Time: 2023-01-03T15:47:19.621374Z, Message: Start pulling container image\n",
      "Kind: Pod, Name: Downloading, Type: Normal, Time: 2023-01-03T15:47:23.295755Z, Message: Start downloading models\n",
      "Kind: Pod, Name: Pulled, Type: Normal, Time: 2023-01-03T15:48:27.029718Z, Message: Container image is pulled successfully\n",
      "Kind: Pod, Name: Downloaded, Type: Normal, Time: 2023-01-03T15:48:27.029718Z, Message: Models are downloaded successfully\n",
      "Kind: Pod, Name: Created, Type: Normal, Time: 2023-01-03T15:48:27.317847Z, Message: Created container inference-server\n",
      "Kind: Pod, Name: Started, Type: Normal, Time: 2023-01-03T15:48:27.428383Z, Message: Started container inference-server\n",
      "Kind: Pod, Name: ContainerReady, Type: Normal, Time: 2023-01-03T15:48:44.88018746Z, Message: Container is ready\n",
      "\n",
      "Container logs:\n",
      "Werkzeug==2.2.2\n",
      "wrapt==1.12.1\n",
      "zipp==3.11.0\n",
      "\n",
      "2023-01-03T15:48:27,955170432+00:00 | gunicorn/run | \n",
      "2023-01-03T15:48:27,957443163+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-03T15:48:27,959365688+00:00 | gunicorn/run | AzureML Inference Server\n",
      "2023-01-03T15:48:27,960972610+00:00 | gunicorn/run | ###############################################\n",
      "2023-01-03T15:48:27,962784734+00:00 | gunicorn/run | \n",
      "2023-01-03T15:48:28,659936912+00:00 | gunicorn/run | Starting AzureML Inference Server HTTP.\n",
      "\n",
      "Azure ML Inferencing HTTP server v0.7.7\n",
      "\n",
      "\n",
      "Server Settings\n",
      "---------------\n",
      "Entry Script Name: /var/azureml-app/06_Model_Deployment/score.py\n",
      "Model Directory: /var/azureml-app/azureml-models/weather-aci-prediction/9\n",
      "Worker Count: 1\n",
      "Worker Timeout (seconds): 300\n",
      "Server Port: 31311\n",
      "Application Insights Enabled: false\n",
      "Application Insights Key: None\n",
      "Inferencing HTTP server version: azmlinfsrv/0.7.7\n",
      "CORS for the specified origins: None\n",
      "\n",
      "\n",
      "Server Routes\n",
      "---------------\n",
      "Liveness Probe: GET   127.0.0.1:31311/\n",
      "Score:          POST  127.0.0.1:31311/score\n",
      "\n",
      "Starting gunicorn 20.1.0\n",
      "Listening at: http://0.0.0.0:31311 (12)\n",
      "Using worker: sync\n",
      "Booting worker with pid: 66\n",
      "Initializing logger\n",
      "2023-01-03 15:48:29,215 | root | INFO | Starting up app insights client\n",
      "logging socket was found. logging is available.\n",
      "logging socket was found. logging is available.\n",
      "2023-01-03 15:48:29,215 | root | INFO | Starting up app insight hooks\n",
      "2023-01-03 15:48:29,372 | root | INFO | Found user script at /var/azureml-app/06_Model_Deployment/score.py\n",
      "2023-01-03 15:48:29,372 | root | INFO | run() is not decorated. Server will invoke it with the input in JSON string.\n",
      "2023-01-03 15:48:29,372 | root | INFO | Invoking user's init function\n",
      "2023-01-03 15:48:29,738 | root | INFO | Users's init has completed successfully\n",
      "2023-01-03 15:48:29,738 | root | INFO | Swaggers are prepared for the following versions: [2, 3].\n",
      "2023-01-03 15:48:29,738 | root | INFO | Scoring timeout setting is not found. Use default timeout: 3600000 ms\n",
      "2023-01-03 15:48:29,739 | root | INFO | AML_FLASK_ONE_COMPATIBILITY is set. Patched Flask to ensure compatibility with Flask 1.\n",
      "2023-01-03 15:50:38,240 | root | INFO | 200\n",
      "127.0.0.1 - - [03/Jan/2023:15:50:38 +0000] \"POST /score HTTP/1.0\" 200 3 \"-\" \"azure-ai-ml/1.2.0 azsdk-python-core/1.26.0 Python/3.8.5 (Linux-5.15.0-1022-azure-x86_64-with-glibc2.10)\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(ml_client.online_deployments.get_logs(\n",
    "    name=\"blue\", endpoint_name=online_endpoint_name, lines=50\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Kind\\tLocation\\tName\")\n",
    "print(\"-------\\t----------\\t------------------------\")\n",
    "for endpoint in ml_client.online_endpoints.list():\n",
    "    print(f\"{endpoint.kind}\\t{endpoint.location}\\t{endpoint.name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[1]'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the blue deployment with some sample data\n",
    "ml_client.online_endpoints.invoke(\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    request_file=\"./model/sample-request.json\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "input_payload = json.dumps({\n",
    "    'data': [[34.927778, 0.24, 7.3899, 83, 16.1000, 1]]\n",
    "})\n",
    "\n",
    "with open('./model/sample-request.json', 'w') as f:\n",
    "    f.write(input_payload)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<azure.core.polling._poller.LROPoller at 0x7f5b1f163f40>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "..................................................................................................."
     ]
    }
   ],
   "source": [
    "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) Define Environment <a id='env'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "Environment(name=\"myenv\")\n",
    "\n",
    "#env = Environment.get(workspace=ws, name=\"AzureML-Minimal\")\n",
    "env = Environment.get(workspace=ws, name=\"AzureML-Minimal\").clone('myenv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pip_package in [\"numpy\", \"onnxruntime\", \"joblib\", \"azureml-core\", \"azureml-monitoring\", \"azureml-defaults\", \"scikit-learn==0.20.3\", \"inference-schema\", \"inference-schema[numpy-support]\"]:\n",
    "    env.python.conda_dependencies.add_pip_package(pip_package)\n",
    "\n",
    "inference_config = InferenceConfig(entry_script='score.py',\n",
    "                                    environment=env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c) Deployment Configuration <a id='configfile'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice\n",
    "\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 1, memory_gb = 1, collect_model_data=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### d) Deploy web service <a id='webservice'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Model(ws, 'model-scaler')\n",
    "model2 = Model(ws, 'support-vector-classifier')\n",
    "\n",
    "service_name = 'weather-aci-prediction'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Model.deploy(ws, service_name, models=[model1, model2], inference_config=inference_config, deployment_config=deployment_config, overwrite=True)\n",
    "service.wait_for_deployment(show_output = True)\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.update(enable_app_insights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### e) Test web service <a id='testservice'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.scoring_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(service.swagger_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "service.state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "input_payload = json.dumps({\n",
    "    'data': [[34.927778, 0.24, 7.3899, 83, 16.1000, 1016.51, 1]],\n",
    "    'method': 'predict'  # If you have a classification model, you can get probabilities by changing this to 'predict_proba'.\n",
    "})\n",
    "\n",
    "output = service.run(input_payload)\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
